{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary libraries.\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import functools\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from IPython.core.display import display, HTML\n",
    "import math \n",
    "\n",
    "np.random.seed(32)\n",
    "\n",
    "\"\"\"\n",
    "Generate data points for plotting the error surface.\n",
    "\"\"\"\n",
    "def calculateErrorSurface(y, x):\n",
    "    # Generate values for parameter space.\n",
    "    N = 200\n",
    "    a0 = np.linspace(-10.0, 14.0, N)\n",
    "    a1 = np.linspace(-10.0, 14.0, N)\n",
    "    a2 = np.linspace(-10.0, 14.0, N)\n",
    "\n",
    "    A0, A1, A2 = np.meshgrid(a0, a1, a2)\n",
    "\n",
    "    # Generate points for plotting the cost-function surface.\n",
    "    J = np.zeros((N,N))\n",
    "    for iter1 in range(0, N):\n",
    "        for iter2 in range(0, N):\n",
    "            yhat = A0[iter1][iter2] + A1[iter1][iter2]*x + A2[iter1][iter2]*x**2\n",
    "            J[iter1][iter2] = (1/M)*np.sum( np.square(y - yhat)  )\n",
    "            \n",
    "    return J, A0, A1, A2\n",
    "            \n",
    "\"\"\"\n",
    "Calculate closed-form solution using the normal equation.\n",
    "\"\"\"\n",
    "def calculateClosedFormSolution(X, x):\n",
    "    # Closed-form solution.\n",
    "    a_opt = np.linalg.pinv(np.transpose(X).dot(X)).dot(np.transpose(X).dot(y))\n",
    "    yhat = a_opt[0, 0] + a_opt[1, 0]*x + a_opt[2, 0]*x**2\n",
    "    Joptimum = (1/M)*np.sum(np.power((y - yhat), 2) )\n",
    "    \n",
    "    return Joptimum, a_opt\n",
    "\n",
    "\"\"\"\n",
    "Batch gradient descent solution.\n",
    "\"\"\"\n",
    "def batchGradientDescent(alpha, n_iterations, X_b, y):\n",
    "    # Random initialization of parameters.\n",
    "    a = np.zeros((3,1))\n",
    "    a[0] = -10;\n",
    "    a[1] = -10;\n",
    "    a[2] = -10;\n",
    "\n",
    "    # Create vector for parameter history.\n",
    "    a_hist = np.zeros((3, n_iterations+1))\n",
    "\n",
    "    # Batch gradient-descent loop.\n",
    "    for iteration in range(n_iterations):\n",
    "        gradients = -2/M * X_b.T.dot(y - X_b.dot(a))\n",
    "        a = a - alpha * gradients\n",
    "        Jgd[iteration+1] = (1/M)*sum(np.power( (y - X.dot(a)) , 2))\n",
    "        \n",
    "    return a, a_hist, Jgd\n",
    "\n",
    "\"\"\"\n",
    "Hypothesis Function\n",
    "\"\"\"\n",
    "def h(X_b, a):\n",
    "    return a.T.dot(X_b.T)\n",
    "    \n",
    "## --------------------------------------------------------   \n",
    "    \n",
    "# Number of examples.\n",
    "M = 1000\n",
    "\n",
    "# Generate target function.\n",
    "x = np.random.uniform(-5, 5, (M, 1))\n",
    "\n",
    "w = np.random.normal(0, math.sqrt(10),(M, 1))\n",
    "y = 2.0 + 1.5*x + 2.3 * x**2 + w\n",
    "\n",
    "X = np.c_[np.ones((M, 1)), x, x**2]\n",
    "X_b = X\n",
    "\n",
    "\n",
    "n_iterations = 1000\n",
    "a = np.zeros((3,1))\n",
    "\n",
    "\n",
    "Jgd = np.zeros(n_iterations+1)\n",
    "\n",
    "Jgd[0] = (1/M)*sum(np.power(y - X.dot(a), 2))\n",
    "\n",
    "# Batch gradient descent solution.\n",
    "alpha = 0.3  # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate data point for plotting error surface.\n",
    "J, A0, A1, A2 = calculateErrorSurface(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate closed-form solution.\n",
    "Joptimum, a_opt = calculateClosedFormSolution(X_b, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run batch gradient-descent algorithm.\n",
    "a, a_hist, Jgd = batchGradientDescent(alpha, n_iterations, X_b, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print found values.\n",
    "print('a0_opt: ' + str(a_opt[0, 0]))\n",
    "print('a1_opt: ' + str(a_opt[1, 0]))\n",
    "print('a2_opt: ' + str(a_opt[2, 0]))\n",
    "print()\n",
    "print('a0_sgd: ' + str(a[0, 0]))\n",
    "print('a1_sgd: ' + str(a[1, 0]))\n",
    "print('a2_sgd: ' + str(a[2, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,12))\n",
    "plt.plot(np.arange(0, n_iterations), Jgd[0:n_iterations])\n",
    "plt.xlim((0, n_iterations))\n",
    "plt.yscale('log')\n",
    "plt.grid(b=True)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('$J_e$')\n",
    "plt.title('Error vs. Iteration number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
